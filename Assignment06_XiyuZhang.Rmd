---
title: "Assignment06_XiyuZhang"
author: "ShaeChang & Jinli Wu"
date: "3/28/2022"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# library all packages needed
```{r library packages}
library(dplyr)
library(rsample)
library(parsnip)
library(recipes)
library(workflows)
library(tune)
library(yardstick)
```

# Exercise 01
```{r Exercise 01}
#To calculate MSE, RMSE, MAE by hand and display the work
knitr::include_graphics("Exercise01.png")
```

As it shows, MSE = 4.4, RMSE = 2.10 (approximately), MAE = 1.6.
RMSE, whose value is larger than MAE, intensified the effect of outliers by taking the square of each difference, while MAE only calculate each absolute value equally. For example, 


# Exercise 02
```{r Exercise 02}
#Using the above data, calculate the following “by hand” and show your work.
knitr::include_graphics("confusionmatrix.png")
knitr::include_graphics("accuracyprecisionrecall.png")
```
The value of accuracy, precision and recall/sensitivity is shown as above.

# Exercise 03
```{r Exercise 03}
#using the data, calculate the following “by hand” and show your work
knitr::include_graphics("exercise03.png")
```

The value of accuracy, misclassification rate is shown as above.

# Exercise 04
```{r Exercise 04}
#to draw the confusion matrix, assume that there are in all 100 observations in the population.
#for the first circumstance, if simply guessing the same value for all observations, then
knitr::include_graphics("guessing4_1.png")
#the accuracy should be 0.51, shown as below:
knitr::include_graphics("accuracy4_1.png")
#same logic, for the second circumstance. The accuracy, in this case, should be 0.99, shown as below.
knitr::include_graphics("accuracy4_2.png")
```
Considering context might reverse our decision that is only based on accuracy. For example, in the situation 2 above, if we predcit all 1, we get accuracy= 0.01, if we predict all 0, we get accuracy=0.99, so we chose to predict all 0. However, if the context is cancer screening:  predicting all 1, we get a lot of false positive but no false negative;  predicting all 0, we get no false positive but some false negative. In the context of cancer screening, false negative means that people who have cancern are not getting treated, and false positive means that people who do have cancern are getting more testing. The cost of false negative is greater than false positive. As such, we want to avoid false negative. In this context, we will choose to predict all 1 rather than predicting all 0. As we can see, considering the context can potentially reverses our original decision based on accuracy. 


# Exercise 05
## Q1
```{r}
marbles_split <- initial_split(data=marbles,prop=0.8)
marbles_split_train <- training(x=marbles_split)
marbles_split_test <- testing(x=marbles_split)
```

## Q2
```{r}
marbles_split_train %>%
  group_by(size,color)%>%
  count()%>%
  ggplot(aes(x=size,y=n,fill=color))+
  geom_bar(stat="identity",position="fill")
```
When we get a big marble, it is more likely it is a black marble, so we will predict it to be black; 
When we get a small marble, it is more likely it is a white marble, so we will predict it to be white.

## Q3
```{r}
prediction <- function(x){
  if_else(x=="big", "black", "white")
}
marbles_split_test$pred_color = factor(prediction(marbles_split_test$size))
marbles_split_test$pred_color
```

## Q4
```{r}
accuracy_confusion <- function(x,y){
  table <- table(x,y)
  table_tibble <- as_tibble(table(x,y))
  accuracy <- (table_tibble[[1,3]]+table_tibble[[4,3]])/sum(table_tibble[,3])
  return(list(accuracy,table))
}
accuracy_confusion(marbles_split_test$color,marbles_split_test$pred_color)
```
## Q5
```{r}
cart_rec <- 
  recipe (formula = color ~ size, data = marbles_split_train)

cart_mod <- 
  decision_tree() %>% 
  set_engine(engine = "rpart") %>% 
  set_mode(mode = "classification")

cart_wf <- 
  workflow() %>% 
  add_recipe(cart_rec) %>%
  add_model(cart_mod)

cart_fit <- 
  cart_wf %>% 
  fit(data = marbles_split_train)
rpart.plot::rpart.plot(x = cart_fit$fit$fit$fit)
```

## Q6
```{r}
Step5 <- predict(object = cart_fit, new_data = marbles_split_test)
identical(marbles_split_test$pred_color, Step5$.pred_class)
```
The decision tree/CART model and the model from part 2 generated the same predictions. These two models have the same outcomes because they use the same underlying rule: for a binary outcome, if the probability of an event occuring exceeds a threshold, then the model predit that event, and the thresholds are set to be 0.5 in both models. 


# Stretch Part 01
```{r Stretch Part01}
set.seed(20200302)
# input the data
rats <- tribble(~rat_burrow, ~pizza_proximity,
  1, 0.01,
  1, 0.05,
  1, 0.08,
  0, 0.1,
  0, 0.12,
  1, 0.2,
  1, 0.3,
  1, 0.5,
  1, 0.75,
  0, 0.9,
  1, 1,
  0, 1.2,
  0, 2.2,
  0, 2.3,
  0, 2.5,
  1, 3,
  0, 3.5,
  0, 4,
  0, 5,
  0, 7
) %>%
 mutate(rat_burrow = factor(rat_burrow)) %>%
  transmute(
    rat_burrow = as.numeric(as.character(rat_burrow)),
    pizza_proximity = pizza_proximity) #transform the factor to numeric form
# split into training and testing data
split <- initial_split(rats, prop = 0.75)
rats_training <- training(split)
rats_testing <- testing(split)
rats_k1 <- vfold_cv(data = rats_training,
                    v = 3)
rats_k3 <- vfold_cv(data = rats_training,
                    v = 3)
rats_kn <- vfold_cv(data = rats_training,
                    v = 3)
#Extract the analysis data and assessment data from the first resample in rats_k1, rats_k3, and rats_kn.
analysis_k1 <- rats_k1$splits[[1]] %>% analysis()
assess_k1 <- rats_k1$splits[[1]] %>% assessment()
analysis_k3 <- rats_k3$splits[[1]] %>% analysis()
assess_k3 <- rats_k3$splits[[1]] %>% assessment()
analysis_kn <- rats_kn$splits[[1]] %>% analysis()
assess_kn <- rats_kn$splits[[1]] %>% assessment()
#calculate y_hat for each assessment data
#for k = 1
# (1) create a knn model specification
knn_mod_k1 <-
  nearest_neighbor(neighbors = 1) %>%
  set_engine(engine = "kknn") %>%
  set_mode(mode = "regression")
# (2) fit the knn model specification on the training data
knn_fit_k1 <- knn_mod_k1 %>%
  fit(formula = pizza_proximity ~ rat_burrow, data = analysis_k1)
# (3) use the estimated model to predict values in the testing data
prediction_k1 <-
  bind_cols(assess_k1,
    predict(object = knn_fit_k1, new_data = assess_k1)
  )
#for k = 3
# (1) create a knn model specification
knn_mod_k3 <-
  nearest_neighbor(neighbors = 3) %>%
  set_engine(engine = "kknn") %>%
  set_mode(mode = "regression")
# (2) fit the knn model specification on the training data
knn_fit_k3 <- knn_mod_k3 %>%
  fit(formula = pizza_proximity ~ rat_burrow, data = analysis_k3)
# (3) use the estimated model to predict values in the testing data
prediction_k3 <-
  bind_cols(assess_k3,
    predict(object = knn_fit_k3, new_data = assess_k3)
  )
#for k = n, let n = 5 because we have 10 observations in total in our analysis data set, and 5 will be used
# (1) create a knn model specification
knn_mod_kn <-
  nearest_neighbor(neighbors = 5) %>%
  set_engine(engine = "kknn") %>%
  set_mode(mode = "regression")
# (2) fit the knn model specification on the training data
knn_fit_kn <- knn_mod_kn %>%
  fit(formula = pizza_proximity ~ rat_burrow, data = analysis_kn)
# (3) use the estimated model to predict values in the testing data
prediction_kn <-
  bind_cols(assess_kn,
    predict(object = knn_fit_kn, new_data = assess_kn)
  )
# Include the data frame in your R Markdown document
knitr::kable(prediction_k1)
knitr::kable(prediction_k3)
knitr::kable(prediction_kn)
# calculate the rmse on the testing data
rmse1 <- rmse(data = prediction_k1, truth = pizza_proximity, estimate = .pred)
rmse3 <- rmse(data = prediction_k3, truth = pizza_proximity, estimate = .pred)
rmsen <- rmse(data = prediction_kn, truth = pizza_proximity, estimate = .pred)
rmse1
rmse3
rmsen

```

To summerize, the rmsen is the smallest (2.07) among all rmse calculated (others are 4.90 and 2.60, respectively), so it seems the last model fit the true values best. Computationally, the first model is the easiest one and the third model is the toughtest, considering the first only need to look up one value to predict and the third one need to look up 5. Note: it seems impossible to calculate the accuracy and the confusion matrix since the result using regression here is numeric but not Boolean. So instead of using the accuracy to estimate the goodness of the model, RMSE is calculated to value the model.

---
title: "assingment07"
author: "Jinli Wu & Xiyu Zhang"
date: "4/8/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse) 
library(tidymodels)
library(lubridate)
library(themis)
library(vip)
library(glmnet)
library(ranger)
library(kknn)
library(yardstick)
library(tune)
library(workflows)
library(recipes)
library(parsnip)
library(rsample)
```

```{r}
# use this url to download the data directly into R
df <- read_csv("https://data.cityofnewyork.us/api/views/43nn-pn8j/rows.csv")

# clean names with janitor
sampled_df <- df %>% 
  janitor::clean_names() 

# create an inspection year variable
sampled_df <- sampled_df %>%
  mutate(inspection_date = mdy(inspection_date)) %>%
  mutate(inspection_year = year(inspection_date))

# get most-recent inspection
sampled_df <- sampled_df %>%
  group_by(camis) %>%
  filter(inspection_date == max(inspection_date)) %>%
  ungroup()

# subset the data
sampled_df <- sampled_df %>%
  select(camis, boro, zipcode, cuisine_description, inspection_date,
         action, violation_code, violation_description, grade,
         inspection_type, latitude, longitude, council_district,
         census_tract, inspection_year, critical_flag) %>%
  filter(complete.cases(.)) %>%
  filter(inspection_year >= 2017) %>%
  filter(grade %in% c("A", "B", "C")) 

# create the binary target variable
sampled_df <- sampled_df %>%
  mutate(grade = if_else(grade == "A", "A", "Not A")) %>%
  mutate(grade = as.factor(grade))

# create extra predictors
sampled_df <- sampled_df %>%
  group_by(boro, zipcode, cuisine_description, inspection_date,
           action, violation_code, violation_description, grade,
           inspection_type, latitude, longitude, council_district,
           census_tract, inspection_year)  %>%
  mutate(vermin = str_detect(violation_description, pattern = "mice|rats|vermin|roaches")) %>%
  summarize(violations = n(),
            vermin_types = sum(vermin),
            critical_flags = sum(critical_flag == "Y")) %>%
  ungroup()

# write the data
write_csv(sampled_df, "restaurant_grades.csv")
```

## Q1-1
```{r}
set.seed(20201020)
df_split <- initial_split(data=sampled_df,prop=0.8)
df_train <- training(df_split)
df_test <- testing(df_split)
cart_rec <- recipe(formula = grade ~ ., data = df_train)%>%
  themis::step_downsample(grade)
cart_mod <-
  decision_tree()%>%
  set_engine(engine = "rpart")%>%
  set_mode(mode = "classification")
cart_wf <- workflow() %>% 
  add_recipe(cart_rec) %>%
  add_model(cart_mod)
cart_fit <- cart_wf %>% 
  fit(data = df_train)
rpart.plot::rpart.plot(x = cart_fit$fit$fit$fit)
```
## Q1-2
```{r}
predictions <- bind_cols(
  df_test, 
  predict(object = cart_fit, new_data = df_test), 
  predict(object = cart_fit, new_data = df_test, type = "prob")
)
conf_mat(data = predictions, truth = grade, estimate = .pred_class)
precision(data = predictions, truth = grade, estimate = .pred_class)
recall(data = predictions, truth = grade, estimate = .pred_class)
```
The precision of the model is .9944107, which means that when model predits the event, there is 99.44107% chance that the prediction is a true positve. As such, the model is excellent at making correct prediction when it predits an event. 
The recall of the model is .7944576, which means when an event actually happens, there is a 79.44576% chance that the model will correctly predict it. As such, the model is moderately good at predicting the true events. 

## Q1-3
We used rpart engine and developed a decision tree for estimating a model. First, we could add auxiliary information, and thus with more features, switch to some regularized regressions, such as ridge regression, or LASSO regression. They could generate an estimation model better with more information while avoid overfitting. Second, we could make better use of current information by better feature engineering. For example, violation_code and violation_description could convert to some more general (comparing to the current violation_code) categorical variables, and may be useful to develop a new node.

## Q1-4
```{r}
cart_fit %>%
  extract_fit_parsnip() %>% 
  vip(num_features = 10)
```
Vip is a package for constructing variable importance scores for many types of supervised learning algorithms. Here, we firstly use the syntax of "extract_fit_parsnip" to return our parsnip model fit. Then we use "vip()" to plot variable importance scores for the predictores in our model. By setting num_features = 10, we specified the number of variable importance scores to plot is 10. Because we use rpart engine, at each node, gini coefficient is used to pure class subsets, so here we are calculating variable importance using gini coefficient as the feature metric by:

Feature importance (FI) = Feature metric * number of instances – its left child node metric * number of instances for the left child – its right child node metric * number of instances for the right child

## Q1-5
One of ombudsman's responsibilities at the NYC health department is to investgiate people's complaints about the sanitary condition of restaurants. The number of complaints usually exceeds what the health department's resources can deal with, so the ombudsman needs to decide which complaints to prioritize in investigation and which ones to postpone (or completely ignore). As new complaints come in, the ombudsman can use this model to predict the probability of that restaurant having a condition of "Not A", and allocate resources based on the  probability.

# Exercise 02

## Q2-1
```{r}

#convert date into useable variable
Chicago$weekday <- wday(Chicago$date, label = T)
Chicago$month <- month(Chicago$date, label = T)
Chicago$yearday <- yday(Chicago$date)

Chicago_modeling <- Chicago %>% 
  slice(1:5678)
Chicago_implementation <- Chicago %>% 
  slice(5679:5698) %>% 
  select(-ridership)

```

## Q2-2
### a
```{r}
#set up a testing environment
set.seed(20211101)
chicago_split <- initial_split(data = Chicago_modeling)
chicago_train <- training(x = chicago_split)
chicago_test <- testing(x = chicago_split)
```

### b
```{r}
#exploratory data analysis

p1 <- ggplot(chicago_train, aes(x = weekday, y = ridership, fill = weekday)) +
  geom_violin() +
  labs(title = "Ridership by weekday") +
  theme_minimal()
  
p2 <- ggplot(chicago_train, aes(x = month, y = ridership, fill = month)) +
  geom_bar(stat="identity") +
  labs(title = "Ridership by month") +
  theme_minimal()
  
p3 <- ggplot(chicago_train, aes(x = yearday, y = ridership, fill = yearday)) +
  geom_bar(stat = "identity") +
  labs(title = "Ridership by year") +
  theme_minimal()
  
library(patchwork)
(p1 | p2)/
  p3
```
It seems that among weekdays, the ridership is usually higher than weekend, and during summer, the ridership is usllay higher than winter. 

```{r}
count(chi_modeling_train, yearday)
```
For p3, we could say the ridership is increasing over time in different years. By counting the observations in different years, we recognized that the drop of ridership in the last year, 2016, is due to the only 169 observations in our dataset, comparing to the around 270 observations in previous years.

### c
```{r}
folds <- vfold_cv(data = chicago_train, v = 10)
folds
```

## Q2-3
### 1
```{r}
chicago_rec <- recipe(formula = ridership ~ ., data= chicago_train) %>%
  step_holiday(date, keep_original_cols = FALSE) %>%
  step_scale(all_numeric_predictors()) %>%
  step_center(all_numeric_predictors()) %>%
  step_unorder(month, weekday) %>%
  step_dummy(month, weekday)
```

### 2,3,4,5

lasso
```{r}
lasso_grid <- grid_regular(penalty(), levels = 10)

lasso_mod <- linear_reg(penalty = tune(), mixture = 1 ) %>% 
  set_engine("glmnet")

lasso_wf <- workflow() %>%
  add_recipe(chicago_rec) %>%
  add_model(lasso_mod) 

lasso_cv <- lasso_wf %>%
  tune_grid(
  resamples = folds,
  grid = lasso_grid
  )

lasso_best <- lasso_cv %>%
  select_best(metric = "rmse")

lasso_final <- finalize_workflow(
  lasso_wf,
  parameters = lasso_best
)

lasso_fit_rs <-
  lasso_final %>% 
  fit_resamples(resamples = folds,
                metric = metric_set(mae, rmse()))

collect_metrics(lasso_fit_rs)

collect_metrics(lasso_fit_rs, summarize = FALSE) %>%
  filter(.metric == "rmse") %>% 
  ggplot(aes(id, .estimate, group = .estimator)) + 
  geom_line() + 
  geom_point() + 
  scale_y_continuous(limits = c(0, 3)) + 
  labs(title = "Calculated RMSE Across the 10 Folds", y = "RMSE_hat") +
  theme_minimal()

```


random forest
```{r}
rforest_mod <- rand_forest(mode = "regression",engine = "ranger")

rforest_wf <- workflow() %>%
  add_recipe(chicago_rec) %>%
  add_model(rforest_mod) 

rforest_rs <-
  rforest_wf %>% 
  fit_resamples(resamples = folds)

collect_metrics(rforest_rs)

collect_metrics(rforest_rs, summarize = FALSE) %>%
  filter(.metric == "rmse") %>% 
  ggplot(aes(id, .estimate, group = .estimator)) + 
  geom_line() + 
  geom_point() + 
  scale_y_continuous(limits = c(0, 3)) + 
  labs(title = "Calculated RMSE Across the 10 Folds", y = "RMSE_hat") +
  theme_minimal()
```

knn
```{r}
knn_mod <-nearest_neighbor(neighbors = 5) %>% 
  set_engine(engine = "kknn") %>% 
  set_mode(mode = "regression")
knn_wf <-workflow() %>% 
  add_recipe(chicago_rec) %>% 
  add_model(knn_mod)
knn_rs <-
  knn_wf %>% 
  fit_resamples(resamples = folds)
collect_metrics(knn_rs)
collect_metrics(knn_rs, summarize = FALSE) %>%
  filter(.metric == "rmse") %>% 
  ggplot(aes(id, .estimate, group = .estimator)) + 
  geom_line() + 
  geom_point() + 
  scale_y_continuous(limits = c(0, 3)) + 
  labs(title = "Calculated RMSE Across the 10 Folds", y = "RMSE_hat") +
  theme_minimal()
```

### 5
```{r}
bind_rows(
  `lasso` = show_best(lasso_fit_rs, metric = "rmse", n = 1),
  `random forest` = show_best(rforest_rs, metric = "rmse", n = 1),
  `knn` = show_best(knn_rs, metric = "rmse",n = 1),
  .id = "model"
)
```

## Q2-4
```{r}
#Estimate the out-of-sample error rate
#since the rmse of lasso model is the lowest one, so we use our lasso_final to fit the testing data.
lasso_fit <- lasso_final %>% 
  fit(data = chicago_train)
predictions <- 
  bind_cols(chicago_test, 
            predict(object = lasso_fit, 
                    new_data = chicago_test))
rmse(data = predictions, truth = ridership, estimate = .pred)
```

## Q2-5
```{r}
#implement the final model
predictions_implement <- 
  bind_cols(Chicago_implementation, 
            predict(object = lasso_fit, 
                    new_data = Chicago_implementation))
print(predictions_implement$.pred)
```
## Q2-5
```{r}

```
